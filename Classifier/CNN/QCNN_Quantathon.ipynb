{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MLnMQoYtJrc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f77024f5-98fa-40a0-b185-691aa92319a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pennylane in /usr/local/lib/python3.11/dist-packages (0.42.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pennylane-lightning-gpu in /usr/local/lib/python3.11/dist-packages (0.42.0)\n",
            "Requirement already satisfied: torchio in /usr/local/lib/python3.11/dist-packages (0.20.19)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.16.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pennylane) (3.5)\n",
            "Requirement already satisfied: rustworkx>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.16.0)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.8.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.11/dist-packages (from pennylane) (1.4.4)\n",
            "Requirement already satisfied: autoray>=0.6.11 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.7.2)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.11/dist-packages (from pennylane) (5.5.2)\n",
            "Requirement already satisfied: pennylane-lightning>=0.42 in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.42.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.32.3)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.11/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from pennylane) (25.0)\n",
            "Requirement already satisfied: diastatic-malt in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.15.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pennylane) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy-openblas32>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu) (0.3.30.0.2)\n",
            "Requirement already satisfied: custatevec-cu12 in /usr/local/lib/python3.11/dist-packages (from pennylane-lightning-gpu) (1.9.0.post0)\n",
            "Requirement already satisfied: deprecated>=1.2 in /usr/local/lib/python3.11/dist-packages (from torchio) (1.2.18)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchio) (0.8.1)\n",
            "Requirement already satisfied: humanize>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (4.12.3)\n",
            "Requirement already satisfied: nibabel>=3 in /usr/local/lib/python3.11/dist-packages (from torchio) (5.3.2)\n",
            "Requirement already satisfied: rich>=10 in /usr/local/lib/python3.11/dist-packages (from torchio) (13.9.4)\n",
            "Requirement already satisfied: simpleitk!=2.0.*,!=2.1.1.1,>=1.3 in /usr/local/lib/python3.11/dist-packages (from torchio) (2.5.2)\n",
            "Requirement already satisfied: tqdm>=4.40 in /usr/local/lib/python3.11/dist-packages (from torchio) (4.67.1)\n",
            "Requirement already satisfied: typer>=0.1 in /usr/local/lib/python3.11/dist-packages (from torchio) (0.16.0)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2->torchio) (1.17.2)\n",
            "Requirement already satisfied: importlib-resources>=5.12 in /usr/local/lib/python3.11/dist-packages (from nibabel>=3->torchio) (6.5.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10->torchio) (2.19.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.1->torchio) (1.5.4)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->pennylane) (2025.7.14)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10->torchio) (0.1.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.11/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pennylane torch torchio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Y6SsSkRufA_",
        "outputId": "0e2a7d53-75a4-4a01-fee7-b6fb986b613f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "shared_folder_path = '/content/drive/MyDrive/Quantum/'\n",
        "!unzip {shared_folder_path + \"mri_simple.zip\"}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX1XVEu6uhAQ",
        "outputId": "624794a3-2ce1-4f71-8c1c-e0bcd2f9ee80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Quantum/mri_simple.zip\n",
            "replace Combined Dataset/test/Mild Impairment/1 (10).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace Combined Dataset/test/Mild Impairment/1 (11).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: nA\n",
            "replace Combined Dataset/test/Mild Impairment/1 (14).jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "transform = transforms.Compose([\n",
        "    transforms.Grayscale(),                # Ensure 1 channel\n",
        "    transforms.Resize((64, 64)),         # Ensure consistent size\n",
        "    transforms.ToTensor(),                 # Converts to shape (1, 128, 128)\n",
        "])\n",
        "\n",
        "train_dataset = datasets.ImageFolder('/content/Combined Dataset/train', transform=transform)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Step 1: Load all data from dataset and move to CUDA\n",
        "all_images = []\n",
        "all_labels = []\n",
        "\n",
        "for img, label in train_dataset:\n",
        "    all_images.append(img.unsqueeze(0))  # add batch dim\n",
        "    all_labels.append(torch.tensor([label]))\n",
        "\n",
        "# Stack all samples into single tensors\n",
        "all_images = torch.cat(all_images).to(device)  # shape: (N, C, H, W)\n",
        "all_labels = torch.cat(all_labels).to(device)  # shape: (N,)\n",
        "\n",
        "# Step 2: Create TensorDataset on CUDA tensors\n",
        "cuda_dataset = TensorDataset(all_images, all_labels)\n",
        "\n",
        "# Step 3: Create DataLoader from cuda_dataset\n",
        "train_loader = DataLoader(cuda_dataset, batch_size=24, shuffle=False)"
      ],
      "metadata": {
        "id": "WIAoVS-Lyjg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XLwsdIQML5_k",
        "outputId": "833434e3-5f52-4dc6-d166-2e1e276d2da9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show train_loader shape\n",
        "for images, labels in train_loader:\n",
        "    print(images.shape)\n",
        "    print(labels)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWFMUwfWyn0Z",
        "outputId": "a7740c4e-eb41-48b3-ee6e-cfa8af2b22d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([24, 1, 64, 64])\n",
            "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import pennylane as qml\n",
        "import numpy as np\n",
        "from pennylane import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set device\n",
        "device = qml.device('lightning.gpu', wires=8, batch_obs=True)\n",
        "# device = qml.device('default.qubit', wires=8)\n",
        "\n",
        "# Manual implementation of ZZFeatureMap\n",
        "def ZZFeatureMap(x, wires, reps=2):\n",
        "    \"\"\"\n",
        "    Manual implementation of ZZFeatureMap encoding\n",
        "    Args:\n",
        "        x: input data (flattened)\n",
        "        wires: list of wire indices\n",
        "        reps: number of repetitions\n",
        "    \"\"\"\n",
        "    n_qubits = len(wires)\n",
        "\n",
        "    for rep in range(reps):\n",
        "        # Single qubit rotations\n",
        "        for i, wire in enumerate(wires):\n",
        "            if i < len(x):\n",
        "                qml.RZ(x[i], wires=wire)\n",
        "\n",
        "        # Entangling gates (ZZ interactions)\n",
        "        for i in range(n_qubits):\n",
        "            for j in range(i + 1, n_qubits):\n",
        "                if i < len(x) and j < len(x):\n",
        "                    # ZZ interaction: exp(-i * x[i] * x[j] * Z_i * Z_j)\n",
        "                    qml.CNOT(wires=[wires[i], wires[j]])\n",
        "                    qml.RZ(x[i] * x[j], wires=wires[j])\n",
        "                    qml.CNOT(wires=[wires[i], wires[j]])\n",
        "\n",
        "# Convolutional ansatz U_SO4\n",
        "def U_SO4(params, wires, noise_model='noiseless', noise_strength=0.0):\n",
        "    \"\"\"\n",
        "    SO4 ansatz for convolutional layer\n",
        "    \"\"\"\n",
        "    qml.RY(params[0], wires=wires[0])\n",
        "    qml.RY(params[1], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.RY(params[2], wires=wires[0])\n",
        "    qml.RY(params[3], wires=wires[1])\n",
        "    qml.CNOT(wires=[wires[0], wires[1]])\n",
        "    qml.RY(params[4], wires=wires[0])\n",
        "    qml.RY(params[5], wires=wires[1])\n",
        "\n",
        "# Pooling ansatz\n",
        "def Pooling_ansatz1(params, wires):\n",
        "    \"\"\"\n",
        "    Pooling ansatz for pooling layer\n",
        "    \"\"\"\n",
        "    qml.CRZ(params[0], wires=[wires[0], wires[1]])\n",
        "    qml.PauliX(wires=wires[0])\n",
        "    qml.CRX(params[1], wires=[wires[0], wires[1]])\n",
        "\n",
        "# Conv and pooling layers\n",
        "def conv_layer(U, params, wires_list):\n",
        "    \"\"\"Apply convolution layer\"\"\"\n",
        "    for wires in wires_list:\n",
        "        U(params, wires=wires)\n",
        "\n",
        "def pooling_layer(V, params, wires_list):\n",
        "    \"\"\"Apply pooling layer\"\"\"\n",
        "    for i, wires in enumerate(wires_list):\n",
        "        V(params[i*2:(i+1)*2], wires=wires)\n",
        "\n",
        "# Improved QCNN structure\n",
        "def qcnn_structure(U_fn, params, U_params, num_classes):\n",
        "    \"\"\"\n",
        "    Improved QCNN structure with better parameter management\n",
        "    \"\"\"\n",
        "    num_measurement_qubits = int(np.ceil(np.log2(num_classes)))\n",
        "    param_idx = 0\n",
        "\n",
        "    # First conv + pooling layer (8 -> 4 qubits)\n",
        "    conv_layer(U_fn, params[param_idx:param_idx + U_params],\n",
        "               [[0, 1], [2, 3], [4, 5], [6, 7]])\n",
        "    param_idx += U_params\n",
        "\n",
        "    pooling_layer(Pooling_ansatz1, params[param_idx:param_idx + 8],\n",
        "                  [[1, 0], [3, 2], [5, 4], [7, 6]])\n",
        "    param_idx += 8\n",
        "\n",
        "    active_wires = [0, 2, 4, 6]\n",
        "    if num_measurement_qubits >= 3:\n",
        "        return active_wires[:int(num_measurement_qubits)], param_idx\n",
        "\n",
        "    # Second conv + pooling layer (4 -> 2 qubits)\n",
        "    conv_layer(U_fn, params[param_idx:param_idx + U_params],\n",
        "               [[0, 2], [4, 6]])\n",
        "    param_idx += U_params\n",
        "\n",
        "    pooling_layer(Pooling_ansatz1, params[param_idx:param_idx + 4],\n",
        "                  [[2, 0], [6, 4]])\n",
        "    param_idx += 4\n",
        "\n",
        "    active_wires = [0, 4]\n",
        "    if num_measurement_qubits == 2:\n",
        "        return active_wires, param_idx\n",
        "\n",
        "    # Third conv + pooling layer (2 -> 1 qubit)\n",
        "    conv_layer(U_fn, params[param_idx:param_idx + U_params],\n",
        "               [[0, 4]])\n",
        "    param_idx += U_params\n",
        "\n",
        "    pooling_layer(Pooling_ansatz1, params[param_idx:param_idx + 2],\n",
        "                  [[4, 0]])\n",
        "    param_idx += 2\n",
        "\n",
        "    return [0], param_idx\n",
        "\n",
        "# Calculate total parameters needed\n",
        "def calculate_total_params(num_classes):\n",
        "    \"\"\"Calculate total number of parameters needed for the QCNN\"\"\"\n",
        "    U_params = 6  # U_SO4 has 6 parameters\n",
        "    total_params = 0\n",
        "\n",
        "    # First layer: 4 conv blocks + 4 pooling blocks\n",
        "    total_params += U_params + 8\n",
        "\n",
        "    # Second layer: 2 conv blocks + 2 pooling blocks\n",
        "    total_params += U_params + 4\n",
        "\n",
        "    # Third layer: 1 conv block + 1 pooling block\n",
        "    total_params += U_params + 2\n",
        "\n",
        "    return total_params\n",
        "\n",
        "# Quantum circuit\n",
        "@qml.qnode(device, interface='torch', diff_method='adjoint')\n",
        "def quantum_circuit(inputs, params, num_classes):\n",
        "    \"\"\"\n",
        "    Complete quantum circuit with encoding and QCNN structure\n",
        "    \"\"\"\n",
        "    # Convert inputs to float32 to ensure compatibility\n",
        "    inputs = inputs.float()\n",
        "    # Data encoding using ZZFeatureMap\n",
        "    ZZFeatureMap(inputs, wires=range(8), reps=1)\n",
        "\n",
        "    # Apply QCNN structure\n",
        "    measurement_wires, _ = qcnn_structure(U_SO4, params, 6, num_classes)\n",
        "\n",
        "    # Measurements\n",
        "    if len(measurement_wires) == 1:\n",
        "        return qml.expval(qml.PauliZ(measurement_wires[0]))\n",
        "    else:\n",
        "        return [qml.expval(qml.PauliZ(wire)) for wire in measurement_wires]\n",
        "\n",
        "# Hybrid quantum-classical model\n",
        "class QCNNModel(nn.Module):\n",
        "    def __init__(self, num_classes=2, input_size=64*64):\n",
        "        super(QCNNModel, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.input_size = input_size\n",
        "\n",
        "        # Calculate quantum parameters\n",
        "        self.num_qparams = calculate_total_params(num_classes)\n",
        "\n",
        "        # Classical preprocessing layers (fixed BatchNorm issue)\n",
        "        self.classical_layers = nn.Sequential(\n",
        "            nn.Linear(input_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(64, 8),  # Reduce to 8 features for 8 qubits\n",
        "            nn.Tanh()  # Bounded activation for quantum circuit\n",
        "        )\n",
        "\n",
        "        # Quantum parameters (better initialization)\n",
        "        self.qparams = nn.Parameter(torch.randn(self.num_qparams, dtype=torch.float32) * 0.1, requires_grad=True)\n",
        "\n",
        "        # Classical post-processing\n",
        "        num_quantum_outputs = int(np.ceil(np.log2(num_classes))) if num_classes > 1 else 1\n",
        "        if num_quantum_outputs == 1:\n",
        "            self.post_processing = nn.Sequential(\n",
        "                nn.Linear(1, 16),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(16, num_classes)\n",
        "            )\n",
        "        else:\n",
        "            self.post_processing = nn.Sequential(\n",
        "                nn.Linear(num_quantum_outputs, 16),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(16, num_classes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1).float()  # Flatten and ensure float32\n",
        "\n",
        "        # Classical preprocessing\n",
        "        x = self.classical_layers(x)\n",
        "\n",
        "        # Process each sample in the batch\n",
        "        quantum_outputs = []\n",
        "        for i in range(batch_size):\n",
        "            # Use processed features directly (already bounded by Tanh)\n",
        "            sample = x[i]\n",
        "\n",
        "            # Quantum circuit\n",
        "            q_out = quantum_circuit(sample, self.qparams, self.num_classes)\n",
        "\n",
        "            if isinstance(q_out, list):\n",
        "                quantum_outputs.append(torch.stack(q_out))\n",
        "            else:\n",
        "                quantum_outputs.append(q_out.unsqueeze(0))\n",
        "\n",
        "        quantum_output = torch.stack(quantum_outputs).float()  # Ensure float32\n",
        "\n",
        "        # Classical post-processing\n",
        "        output = self.post_processing(quantum_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Visualization function\n",
        "def visualize_dataset_samples(dataset, num_samples=8, figsize=(12, 8)):\n",
        "    \"\"\"\n",
        "    Visualize sample images from the dataset\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 4, figsize=figsize)\n",
        "    axes = axes.ravel()\n",
        "\n",
        "    # Get class names\n",
        "    class_names = dataset.classes if hasattr(dataset, 'classes') else [f'Class {i}' for i in range(len(dataset.class_to_idx))]\n",
        "\n",
        "    for i in range(min(num_samples, len(dataset))):\n",
        "        # Get random sample\n",
        "        idx = np.random.randint(0, len(dataset))\n",
        "        image, label = dataset[idx]\n",
        "\n",
        "        # Convert tensor to numpy and handle single channel\n",
        "        if image.dim() == 3:\n",
        "            img_np = image.squeeze().numpy()\n",
        "        else:\n",
        "            img_np = image.numpy()\n",
        "\n",
        "        axes[i].imshow(img_np, cmap='gray')\n",
        "        axes[i].set_title(f'{class_names[label]}')\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    # Hide unused subplots\n",
        "    for i in range(num_samples, len(axes)):\n",
        "        axes[i].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Dataset analysis function\n",
        "def analyze_dataset(dataset):\n",
        "    \"\"\"\n",
        "    Analyze dataset distribution and properties\n",
        "    \"\"\"\n",
        "    print(\"=== Dataset Analysis ===\")\n",
        "    print(f\"Total samples: {len(dataset)}\")\n",
        "\n",
        "    if hasattr(dataset, 'classes'):\n",
        "        print(f\"Number of classes: {len(dataset.classes)}\")\n",
        "        print(f\"Class names: {dataset.classes}\")\n",
        "\n",
        "        # Count samples per class\n",
        "        from collections import Counter\n",
        "        labels = [dataset[i][1] for i in range(len(dataset))]\n",
        "        class_counts = Counter(labels)\n",
        "\n",
        "        print(\"Class distribution:\")\n",
        "        for i, class_name in enumerate(dataset.classes):\n",
        "            count = class_counts.get(i, 0)\n",
        "            percentage = (count / len(dataset)) * 100\n",
        "            print(f\"  {class_name}: {count} samples ({percentage:.1f}%)\")\n",
        "\n",
        "    # Sample image info\n",
        "    sample_image, sample_label = dataset[0]\n",
        "    print(f\"\\nSample image shape: {sample_image.shape}\")\n",
        "    print(f\"Image dtype: {sample_image.dtype}\")\n",
        "    print(f\"Value range: [{sample_image.min():.3f}, {sample_image.max():.3f}]\")\n",
        "\n",
        "    return len(dataset.classes) if hasattr(dataset, 'classes') else None\n",
        "def train_qcnn(model, train_loader, num_epochs=10, learning_rate=0.01):\n",
        "    \"\"\"\n",
        "    Training function for QCNN model with improved optimization\n",
        "    \"\"\"\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Separate optimizers for classical and quantum parameters\n",
        "    classical_params = [p for name, p in model.named_parameters() if 'qparams' not in name]\n",
        "    quantum_params = [model.qparams]\n",
        "\n",
        "    classical_optimizer = optim.Adam(classical_params, lr=learning_rate)\n",
        "    quantum_optimizer = optim.Adam(quantum_params, lr=learning_rate * 10)  # Higher LR for quantum\n",
        "\n",
        "    # Learning rate schedulers\n",
        "    classical_scheduler = optim.lr_scheduler.StepLR(classical_optimizer, step_size=3, gamma=0.7)\n",
        "    quantum_scheduler = optim.lr_scheduler.StepLR(quantum_optimizer, step_size=3, gamma=0.7)\n",
        "\n",
        "    model.train()\n",
        "    losses = []\n",
        "    accuracies = []\n",
        "\n",
        "    # Get number of classes from model\n",
        "    num_classes = model.num_classes\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            # Validate targets are within valid range\n",
        "            if torch.any(target >= num_classes) or torch.any(target < 0):\n",
        "                print(f\"Warning: Invalid target values found. Expected range [0, {num_classes-1}], got {target}\")\n",
        "                # Clip targets to valid range\n",
        "                target = torch.clamp(target, 0, num_classes-1)\n",
        "\n",
        "            classical_optimizer.zero_grad()\n",
        "            quantum_optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output = model(data)\n",
        "            loss = criterion(output, target)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            classical_optimizer.step()\n",
        "            quantum_optimizer.step()\n",
        "\n",
        "            # Statistics\n",
        "            total_loss += loss.item()\n",
        "            _, predicted = torch.max(output.data, 1)\n",
        "            total += target.size(0)\n",
        "            correct += (predicted == target).sum().item()\n",
        "\n",
        "            if batch_idx % 10 == 0:  # Less frequent printing\n",
        "                print(f'Epoch: {epoch+1}/{num_epochs}, Batch: {batch_idx}, '\n",
        "                      f'Loss: {loss.item():.4f}')\n",
        "\n",
        "        # Step schedulers\n",
        "        classical_scheduler.step()\n",
        "        quantum_scheduler.step()\n",
        "\n",
        "        epoch_loss = total_loss / len(train_loader)\n",
        "        epoch_acc = 100. * correct / total\n",
        "\n",
        "        losses.append(epoch_loss)\n",
        "        accuracies.append(epoch_acc)\n",
        "\n",
        "        print(f'Epoch: {epoch+1}/{num_epochs}, '\n",
        "              f'Average Loss: {epoch_loss:.4f}, '\n",
        "              f'Accuracy: {epoch_acc:.2f}%')\n",
        "        print(f'Classical LR: {classical_scheduler.get_last_lr()[0]:.6f}, '\n",
        "              f'Quantum LR: {quantum_scheduler.get_last_lr()[0]:.6f}')\n",
        "\n",
        "    return losses, accuracies\n",
        "\n"
      ],
      "metadata": {
        "id": "vCC0ONfKxwle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "num_classes = 4  # Sesuaikan dengan jumlah kelas dalam dataset Anda\n",
        "model = QCNNModel(num_classes=num_classes).to('cuda')\n",
        "\n",
        "print(f\"Model initialized with {sum(p.numel() for p in model.parameters())} parameters\")\n",
        "print(f\"Quantum parameters: {model.num_qparams}\")\n",
        "\n",
        "# Training (uncomment untuk training dengan data asli)\n",
        "print(\"train_loader\", train_loader.dataset[0][0].device)\n",
        "losses, accuracies = train_qcnn(model, train_loader, num_epochs=5, learning_rate=0.01)\n",
        "\n",
        "print(\"QCNN model berhasil dibuat!\")\n",
        "print(\"Untuk menggunakan dengan dataset Anda, uncomment bagian train_dataset dan train_loader\")\n",
        "torch.save(model.state_dict(), shared_folder_path + \"qcnn.pth\")"
      ],
      "metadata": {
        "id": "3wft4d5dhUAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model to the drive\n",
        "torch.save(model.state_dict(), shared_folder_path + \"qcnn.pth\")"
      ],
      "metadata": {
        "id": "CrYbknkqg7Y3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}